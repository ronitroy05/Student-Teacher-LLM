{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer, LlamaConfig, TrainingArguments, Trainer\nimport torch\nimport shutil\nfrom datasets import load_dataset\nimport torch.nn.functional as F\nimport os\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load TinyLlama as the teacher model\nteacher_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nteacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name).to(device)\nteacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n\n\nstudent_config = LlamaConfig(\n    vocab_size=teacher_model.config.vocab_size,\n    hidden_size=128,\n    intermediate_size=256,\n    num_hidden_layers=2,\n    num_attention_heads=2,\n    max_position_embeddings=512,\n    rms_norm_eps=1e-6,\n    tie_word_embeddings=True\n)\n\nstudent_model = LlamaForCausalLM(student_config).to(device)\nstudent_tokenizer = teacher_tokenizer\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"ðŸ§  Student model parameter count: {count_parameters(student_model):,}\")\n\n\ntrain_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\neval_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n\ndef tokenize_function(examples):\n    return student_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\neval_dataset = eval_dataset.map(tokenize_function, batched=True)\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\neval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"custom-llm-student\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    num_train_epochs=2,\n    logging_dir=\"./logs\",\n    fp16=True if torch.cuda.is_available() else False,\n    run_name=\"custom_llm_training\",\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n# Distillation loss function\ndef distillation_loss(student_logits, teacher_logits, temperature=2.0):\n    student_probs = F.log_softmax(student_logits / temperature, dim=-1)\n    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n    return F.kl_div(student_probs, teacher_probs, reduction=\"batchmean\")\n\n# Trainer with knowledge distillation\nclass DistillationTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        inputs = {key: val.to(device) for key, val in inputs.items()}\n        student_outputs = model(**inputs)\n        with torch.no_grad():\n            teacher_outputs = teacher_model(**inputs)\n        loss = distillation_loss(student_outputs.logits, teacher_outputs.logits)\n        return (loss, student_outputs) if return_outputs else loss\n\ntrainer = DistillationTrainer(\n    model=student_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T15:37:27.853255Z","iopub.execute_input":"2025-04-06T15:37:27.853582Z","iopub.status.idle":"2025-04-06T18:48:26.130093Z","shell.execute_reply.started":"2025-04-06T15:37:27.853548Z","shell.execute_reply":"2025-04-06T18:48:26.129102Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"207470c8571d48d68ee70f2beae73857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01f39b558fae482b85475d170e5651b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6353887fc6994e8dac712acbc8a6c871"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986d1d6d074840b0aab00e1f0e4db5c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e7a2d9747de4130adb06734fad87444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b00274e3c7412983f32a7c4d679e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14031657dc254dff8fc4cdbe4755ddb2"}},"metadata":{}},{"name":"stdout","text":"ðŸ§  Student model parameter count: 4,424,320\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e42980992de4d098648a04f0674e77f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aa82aa71f2640bfb117ac1297f64cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29df7e52fc3e48d5b50c3ab18ab35172"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"172d4d10d4b44925ac7a14049f7b8c2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89e0576fcd164fe9b39628102f74fce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0266930163114bb9880a7ca9310c9d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001d1ac600c144edb95b69f869d585ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee2507bc6dd4f5989d7aae3f05aa081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db74e24819544b99d14a96027801027"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1146/1146 3:09:40, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>368.438300</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>293.046700</td>\n      <td>No log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1146, training_loss=323.9746911812827, metrics={'train_runtime': 11391.7115, 'train_samples_per_second': 6.446, 'train_steps_per_second': 0.101, 'total_flos': 37003481579520.0, 'train_loss': 323.9746911812827, 'epoch': 1.9986928104575163})"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"\noutput_dir = \"custom-llm-student\"\nstudent_model.save_pretrained(output_dir)\nstudent_tokenizer.save_pretrained(output_dir)\nshutil.make_archive(output_dir, 'zip', output_dir)\nprint(\"âœ… Training complete â€” model saved and zipped!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:34:17.295151Z","iopub.execute_input":"2025-04-08T18:34:17.295468Z","iopub.status.idle":"2025-04-08T18:34:17.304563Z","shell.execute_reply.started":"2025-04-08T18:34:17.295441Z","shell.execute_reply":"2025-04-08T18:34:17.303517Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-eae80f3f1f52>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save and export the trained modelqZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"custom-llm-student\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'student_model' is not defined"],"ename":"NameError","evalue":"name 'student_model' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"def upload_folder_to_s3(local_folder: str, bucket_name: str, s3_prefix: str):\n    s3 = boto3.client(\"s3\")\n    local_folder_path = Path(local_folder)\n    for root, _, files in os.walk(local_folder_path):\n        for file in files:\n            local_path = Path(root) / file\n            rel_path = local_path.relative_to(local_folder_path)\n            s3_key = str(Path(s3_prefix) / rel_path)\n            s3.upload_file(str(local_path), bucket_name, s3_key)\n            logger.info(f\"Uploaded {local_path} to s3://{bucket_name}/{s3_key}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}